def sentence_tokenizer(text):
    return text.split('.')

def tokenizer(text):
    return text.split(' ')
